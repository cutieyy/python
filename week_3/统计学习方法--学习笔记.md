# 统计学习方法---第一章（统计学习方法概论）

叙述统计学习方法的一些基本概念

## 1.1统计学习

1）特点（数据，方法，模型，预测，分析）

2）对象（数据）

3）目的（通过构建概率统计模型对已知或未知数据进行预测与分析）

4）方法（由监督学习、非监督学习、半监督学习、强化学习等组成）

三要素：模型、策略（评价标准）、算法

实现统计学习方法的步骤：

（1）得到一个有限的训练数据集合；

（2）确定包含所有可能的模型的假设空间，即学习模型的集合；

（3）确定模型选择的准则，即学习的策略；

（4）实现求解最有模型的算法，即学习的算法；

（5）通过学习方法选择最优模型；

（6）利用学习的最优模型对新数据进行预测或分析。

5）研究（统计学习方法、统计学习理论、统计学习应用）

6）重要性

## 1.2 监督学习（学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测）

### 1.2.1基本概念

1）输入空间、输出空间、特征空间

输入变量与输出变量均为连续变量的预测问题称为回归问题；输出变量为有限个离散变量的预测问题称为分类问题；输入变量与输出变量均为变量序列的预测问题称为标注问题。

2）联合概率分布

监督学习关于数据的基本假设：X和Y具有联合概率分布

3）假设空间

模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。

### 1.2.2 问题的形式化

监督学习分为学习和预测两个过程。

## 1.3 统计学习三要素（方法=模型+策略+算法）

### 1.3.1模型

在监督学习过程中，模型就是所要学习的条件概率分布或决策函数，模型的假设空间包含所有可能的条件概率分布或决策函数（非概率）。

![屏幕快照 2018-12-03 上午11.49.21](/Users/momo/Desktop/屏幕快照 2018-12-03 上午11.49.21.png)

### 1.3.2策略

1）损失函数和风险函数（损失函数度量一次预测的好坏，风险函数度量平均意义下模型预测的好坏）

常用的损失函数：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数

根据期望风险最小模型要用到联合分布，但联合分布又是未知的，所以监督学习就成为一个病态问题，但根据大数定律，当样本容量趋于无穷大时，经验风险趋于期望风险，故可以用经验风险代替期望风险，但由于实际样本数目有限，故要对经验风险进行一定的矫正，这就关系到两个基本策略，经验风险最小化和结构风险最小化。

2）经验风险最小化（ERM）与结构风险最小化（SRM）

当样本容量足够大时，经验风险最小化能保证有很好的学习效果。极大似然估计就是经验风险最小化的一个例子。（当模型是条件概率分布，损失函数是对数损失函数时，两者等价）

当样本容量很小时，经验风险最小化学习的效果容易产生过拟合现象。因此为了防止过拟合提出了结构风险最小化（SRM）

结构风险最小化等价于正则化，在经验风险上加上表示模型复杂度的正则化项或罚项。

贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子。（当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，两者等价）

故监督学习问题变成了经验风险或结构风险函数的最优化问题。

### 1.3.3算法

统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。

## 1.4 模型评估与模型选择

### 1.4.1训练误差与测试误差

（测试误差小的方法具有更好的预测能力，是更有效的方法）

### 1.4.2过拟合与模型选择

（模型选择旨在避免过拟合并提高模型的预测能力）

当模型的复杂度增加时，训练误差会逐渐减小并趋向于0，而测试误差会先减小，达到最小值后增大。当选择的模型复杂度过大时，过拟合现象就会出现。因此在学习时要防止过拟合，选择复杂度适当的模型，以达到使测试误差最小的学习目的。可用的两种方法是：正则化与交叉验证。

## 1.5 正则化与交叉验证（两种模型选择的方法）

### 1.5.1正则化

正则化是结构风险最小化策略的实现，是在经验风险上加上一个正则化项或罚项。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。正则化项可以是模型参数向量的范数。

### 1.5.2交叉验证

如果给定的样本数据充足，进行模型选择时的一种简单方法是随机的将数据集切分成三部分，分别为训练集、验证集、测试集。在学习到的不同复杂度的模型中，选择对验证集有最小预测误差的模型。

如果数据不充足，可采用交叉验证方法。

1）简单交叉验证：首先随机的将已给数据分为两部分，一部分作为训练集，另一部分作为测试集；然后用训练集在各种条件下训练模型，从而得到不同的模型；在测试集上评价各个模型的测试误差，选出测试误差最小的模型。

2）S折交叉验证：首先将已给的数据切分成S个互不相交的大小相同的子集；然后利用S-1个子集的数据训练模型，利用余下的子集测试模型；将这一过程对可能的S种选择重复进行；最后选出S次评测中平均测试误差最小的模型。

3）留一交叉验证：S折交叉验证的特殊情况是S=N，称为留一交叉验证，这里的N是给定数据集的容量。

## 1.6 泛化能力

### 1.6.1泛化误差

事实上，泛化误差就是所学习到的模型的期望风险。

### 1.6.2泛化误差上界

通过比较两种方法的泛化误差上界的大小来比较他们的优劣。

## 1.7 生成模型与判别模型

监督学习方法可以分为生成方法和判别方法。所学到的模型分别称为生成模型和判别模型。

1）生成方法：由数据学习联合概率分布，然后求出条件概率分布作为预测的模型。典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型。

​        特点：生成方法可以还原出联合概率分布，而判别方法则不行；生成方法的学习收敛速度更快；当存在隐变量时，仍可以用生成方法学习，此时判别方法不能用。

2）判别方法：由数据直接学习决策函数或者条件概率分布作为预测模型，即判别模型。典型模型有：K近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。

​        特点：直接面对预测，往往学习的准确率更高；直接学习可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

## 1.8 分类问题

1）分类问题包括学习和分类两个过程。

2）评价分类器性能的指标一般是分类准确率。对于二分类问题常用的评价指标是精确率和召回率。

3）可以用于分类的统计学方法有：K近邻法、感知机、朴素贝叶斯法、决策树、决策列表、逻辑斯蒂回归模型、支持向量机、提升方法、贝叶斯网络、神经网络、Winnow等。

## 1.9 标注问题

1）标注问题也是一个监督学习问题。输入是一个观测序列，输出是一个标记序列或状态序列。

2）标注问题分为学习和标注两个过程。

3）评价标注模型的指标与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。

4）标注常用的统计方法有：隐马尔可夫模型、条件随机场。

## 1.10 回归问题

1）回归用于预测输入变量和输出变量之间的关系。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好的预测未知数据。

2）回归问题分为学习和预测两个过程。

3）回归问题按照输入变量的个数分为一元回归和多元回归；按照输入变量和输出变量之间的关系的类型即模型的类型分为线性回归和非线性回归。

4）回归学习常用的损失函数是平方损失函数，在此情况下，回归问题可以由最小二乘法求解。

# 例题：（最小二乘法）

使用最小二乘法拟合曲线：

对于数据(xi,yi)(i=1,2,3...,m)拟合出函数h(x)；有误差，即残差：ri=h(xi)−yi；此时L2范数(残差平方和)最小时，h(x) 和 y 相似度最高，更拟合。一般的H(x)为n次的多项式，H(x)=w0+w1x+w2x2+...wnxn；w(w0,w1,w2,...,wn)为参数

最小二乘法就是要找到一组 w(w0,w1,w2,...,wn)使得∑ni=1(h(xi)−yi)2 (残差平方和) 最小即，求 min∑ni=1(h(xi)−yi)2

# 第二章（感知机）

1）感知机是二类分类的线性分类模型。其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。

2）感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。

3）感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。

4）感知机学习算法分为原始形式和对偶形式。

## 2.1感知机模型

1）由输入空间到输出空间的如下函数：
$$
f(x)=sign(w*x+b)
$$
称为感知机。

2）sign是符号函数，即：
$$
sign(x)=+1,  x>=0  ; sign(x)=-1,  x<0
$$
3）感知机模型的假设空间是定义在特征空间中的所有线性分类模型，即函数集合：
$$
{f|f(x)=w*x+b}
$$
4）线性方程
$$
w*x+b=0
$$
对应于特征空间中的一个超平面S，这个超平面将特征空间划分为两个部分，因此超平面S称为分离超平面。

## 2.2感知机学习策略

### 2.2.1数据集的线性可分性

对所有y_i=+1的实例i，有w*x_i+b>0，对所有y_i=-1的实例i，有w*x_i+b<0，则称该数据集为线性可分数据集。

### 2.2.2感知机学习策略

为了求得分离超平面，即确定感知机模型参数w,b；需要一个学习策略，即定义（经验）损失函数并将损失函数最小化。

1）感知机所采取的损失函数是误分类点到超平面S的总距离。

任意一点x_0到超平面的距离：
$$
|w*x_0+b|/||w||
$$
对于误分类的数据来说，
$$
-y_i(w*x_i+b)>0
$$
成立，因此误分类点到超平面S的距离是：
$$
-y_i(w*x_i+b)/||w||
$$
这样，假设误分类点的集合为M，则所有误分类点到超平面的距离为：
$$
-1/||w||*sigma y_i(w*x_i+b)
$$
不考虑1/||w||，就得到感知机学习的损失函数：
$$
-sigma y_i(w*x_i+b)
$$
误分类点越少，误分类点离超平面越近，损失函数就越小。感知机学习的策略是在假设空间中选取使损失函数最小的模型参数w,b，即感知机模型。

## 2.3感知机学习算法

感知机学习问题转化为求解损失函数的最优化问题，最优化的方法是随机梯度下降法。

### 2.3.1感知机学习算法的原始形式

利用梯度下降法求解使得损失函数最小的参数w,b；极小化过程中不是一次使用M中所有的误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

算法2-1：

输入：训练数据集T={(x_1,y_1),(x_2,y_2),......(x_N,y_N)}，其中x_i属于x=R^n，y_i属于y={-1,+1}，i=1,2,3,.....N;学习率q(0<q<=1)；

输出：w,b；感知机模型f(x)=sign(w*x+b)

（1）选取初值w_0,b_0

（2）在训练集中选取数据（x_i,y_i）

（3）如果y_i(w*x_i+b)<=0


$$
w=w+q*y_i*x_i；b=b+q*y_i
$$
（4）转至（2），直至训练集中没有误分类点。

算法解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

### 2.3.2算法的收敛性

1）通过证明可知，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。

2）即当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。

3）感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。

4）当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。

### 2.3.3感知机学习算法的对偶形式

算法2.2（感知机学习算法的对偶形式）

输入：线性可分的数据集T={(x_1,y_1),(x_2,y_2),....(x_N,y_N)}，其中x_i属于R^n，y_i属于{-1,+1},i=1,2,.....N；学习率q(0<q<=1)；

输出：a,b；感知机模型f(x)=sign(sigma a_jy_jx_j*x+b)

其中a=(a_1,a_2,.....a_N)^T

（1）a=0, b=0

（2）在训练集中选取数据（x_i,y_i)

（3）如果y_i*(sigma a_jy_j*x_j*x_i+b)<=0，则

a_i=a_i+q ，b=b+q*y_i

（4）转至（2）直到没有误分类数据。

 感知机学习算法的对偶形式迭代是收敛的，存在多个解。

# 第三章（k近邻法）

1）k近邻是一种基本分类与回归方法。

2）k近邻的输入为实例的特征向量，对应于特征空间的点，输出为实例的类别，可以取多类。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决的方式进行预测。

3）k值的选取、距离的度量、分类的决策规则是k近邻法的三个基本要素。

## 3.1 k近邻算法

1）给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。

2）算法3.1（K近邻法）

输入：训练数据集。

输出：实例x所属的类y。

（1）根据给定的距离度量，在训练集中找出与x最邻近的K个点，涵盖这k个点的x的邻域记作N_k(x)；

（2）在N_k(x)中根据分类决策规则（如多数表决）决定x的类别y。

k近邻的特殊情况是k=1的情形，称为最近邻算法。对于输入的实例点x，最近邻法将训练数据集中与x最邻近点的类作为x的类。

## 3.2 k近邻模型

### 3.2.1模型

1）k近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，他所属的类唯一的确定。

### 3.2.2距离度量

特征空间中两个实例点的距离是两个实例点相似程度的反映。由不同的距离度量所确定的最邻近点是不同的。

### 3.2.3k值的选择

1）选取较小的k值，就相当于用较小的邻域中的训练实例进行预测。k值的减小就意味着整体模型变得复杂，容易发生过拟合。

2）选取较大的k值，就相当于用较大的邻域中的训练实例进行预测。k值的增大就意味着整体模型变得简单。

3）在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的K值。

### 3.2.4分类决策规则

K近邻法中的分类规则一般选择多数表决。（等价于经验风险最小化）

## 3.3 k近邻法的实现（kd树）

实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。最简单的实现方法是线性扫描，这时要计算输入实例与每个训练实例的距离，计算非常耗时，为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储数据，以减少计算距离的次数。——kd树方法

### 3.3.1构造kd树

1）kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分。

2）算法3.2（构造平衡kd树）

输入：k维空间数据集T；

输出：kd树。

（1）开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域。

选择x^(1)为坐标轴，以T中所有实例的x^(1)坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x^(1)垂直的超平面实现。

由根节点生成深度为1的左右子结点：左子结点对应于左边x^(1)小于切分点的子区域，右子结点对应于左边x^(1)大于切分点的子区域。将落在切分超平面上的实例点保存在根节点。

（2）重复：对深度为j的结点，选择x^(L)为切分的坐标轴，L=j(modk)+1,以该结点的区域中的所有实例的x^(L)坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x^(L)垂直的超平面实现。

由该节点生成深度为j+1的左右子节点：左子结点对应于坐标x^(1)小于切分点的子区域，右子节点对应于坐标x^(1)大于切分点的子区域。

将落在切分超平面上的实例点保存在该结点。

（3）直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。

### 3.3.2搜索kd树

算法3.3（用kd树的最近邻搜索）

输入：已构造的kd树；目标点x；

输出：x的最近邻。

（1）在kd树中找出包含目标点x的叶节点；从根节点出发，递归的向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到自己诶单为叶节点为止。

（2）以此叶节点为“当前最近点”。

（3）递归的向上回退，在每个节点进行以下操作：

（a）如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。

（b）当前最近点一定存在于该节点一个子节点对应的区域。检查该子节点的父节点的另一个子节点对应的区域是否有更近的点。具体的，检查另一个子节点对应的区域是否与以目标为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。

如果相交，可能在另一个子节点对应的区域内存在距目标点更近的点，移动到另一个子节点。接着递归的进行最近邻搜索。

如果不相交，向上回退。

（4）当回退到根节点时，搜索结束。最后的“当前最近点”即为x的最近邻点。

# 第四章 朴素贝叶斯法

## 4.1朴素贝叶斯法的学习与分类

### 4.1.1 基本方法

1）朴素贝叶斯法实际上学习到生成数据的机制，所以属于生成模型。条件独立的假设等于是说用于分类的特征在类确定的条件下都是条件独立的。

### 4.1.2 后验概率最大化的含义

1）朴素贝叶斯法采用的原理，根据期望风险最小化准则得到后验概率最大化准则。

## 4.2朴素贝叶斯法的参数估计

### 4.2.1极大似然估计

1）在朴素贝叶斯法中，学习意味着估计先验概率和条件概率，可以应用极大似然估计法估计相应的概率。

### 4.2.2学习与分类算法

1）算法4.1（朴素贝叶斯算法）

### 4.2.3贝叶斯估计

1）利用极大似然估计可能会出现所要估计的概率值为0的情况，这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法就是采用贝叶斯估计。

2）贝叶斯估计即在随机变量各个取值的频数上赋予一个正数，当该正数为0时，即为极大似然估计。

###### 本章概要：

1）朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布，然后求得后验概率分布。具体来说利用数据学习先验概率和条件概率，得到联合概率分布。概率估计的方法可以是极大似然估计或贝叶斯估计。

2）朴素贝叶斯法的基本假设是条件独立性。

3）朴素贝叶斯法利用贝叶斯定理与学习的到的概率模型进行分类预测。将输入x分到后验概率最大的类中。后验概率最大等价于0-1损失函数时的期望风险最小化。

4）朴素贝叶斯法中假设输入变量都是条件独立的，如果假设他们之间存在概率依存关系，模型就变成了贝叶斯网络。

# 第五章 决策树

1）决策树是一种基本的分类与回归方法。

2）学习时，利用训练数据，根据损失函数最小化原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。

3）决策树学习时通常包括3个步骤：特征选择、决策树生成、决策树修剪。

## 5.1 决策树模型与学习

### 5.1.1决策树模型

分类决策树模型是一种描述对实例进行分类的树形结构。

### 5.1.2决策树与if-then规则

每一个实例都被一条路径或一条规则所覆盖。

### 5.1.3决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。

### 5.1.4决策树学习

1）决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择，决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。

2）决策树学习常用的三种算法有：ID3、C4.5与CART。

## 5.2特征选择

### 5.2.1特征选择问题

1）特征选择在于选取对训练数据集具有分类能力的特征。这样可以提高学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大的差别，则称这个特征是没有分类能力的。通常特征选择的准则是信息增益或信息增益比。

### 5.2.2信息增益

### 5.2.3信息增益比

## 5.3决策树的生成

### 5.3.1ID3算法

1）ID3算法的核心是在决策树各个节点上应用信息增益准则选择特征，递归的构建决策树。

2）具体方法：从根节点开始，对节点计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，由该特征的不同取值建立子节点；再对子节点递归的调用以上的方法，构建决策树；直到所有特征的信息增益均很小或者没有特征可以选择为止。最有得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。

3）ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。

### 5.3.2 C4.5的生成算法

该算法与ID3算法类似，是对ID3算法的改进。C4.5在生成的过程中，是用信息增益比来选择特征。

## 5.4决策树的剪枝

1）剪枝从已生成的树上裁掉一些子树或叶节点，将其根节点或父节点作为新的叶节点，从而简化分类树模型。

2）决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。

3）决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部模型，而决策树剪枝学习整体的模型。

4）利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。

5）决策树的剪枝算法可以由一种动态规划的算法实现。

## 5.5CART算法

1）分类与回归模型树

2）CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。

3）CART假设决策树是二叉树。

4）CART算法由以下两步组成：

​           （1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；

​           （2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### 5.5.1CART生成

1）决策树生成就是递归的构建二叉树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，，生成二叉树。

2）回归树的生成

最小二乘回归树生成算法：

3）分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。

4）CART生成算法

### 5.5.2CART剪枝

1）CART剪枝算法由两步组成：首先从生成算法产生的决策树T0底端开始不断剪枝，直到T0的根节点，形成一个子树序列{T0,T1,....Tn}；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。



【本章概要】

决策树特征选择：关键是利用什么准则（信息增益、信息增益比、基尼指数）

决策树生成：通常使用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。

决策树剪枝：由于生成的决策树存在过拟合问题，需要对它进行剪枝。





# 第六章 逻辑斯谛回归与最大熵模型











 















































